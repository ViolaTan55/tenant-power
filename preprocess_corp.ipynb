{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fiona\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyArango\n",
    "import usaddress\n",
    "from usaddress import tag\n",
    "from scourgify import normalize_address_record\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyArango.connection import *\n",
    "conn = Connection(username=\"root\", password=\"0505\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PG_CONNECT = os.getenv(\"PG_CONNECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arangoDB database, or open it if it already exists\n",
    "try:\n",
    "    corp_db = conn.createDatabase(name=\"corp_data\")\n",
    "    # from command line:\n",
    "    # arangoimport --file '''path to CorpData.csv''' --collection corp_data --create-collection true --type csv --server.database corp_data\n",
    "    # arangoimport --file '''path to CorpIndividualExport.csv''' --collection corp_individual --create-collection true --type csv --server.database corp_data\n",
    "\n",
    "except:\n",
    "    corp_db = conn[\"corp_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** start of ONLY RUN ONCE for corp_data *****\n",
    "# process the raw corp_data collection, including normalization of names, addresses, and slicing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the values of the corp_data collection\n",
    "val_aql = \"FOR x IN corp_data RETURN x\" # here, corp_data is the name of the collection, not the database\n",
    "value_query_result = corp_db.AQLQuery(val_aql,rawResults=True, batchSize = 1000)\n",
    "col_value = {}\n",
    "ind_val = 0\n",
    "\n",
    "for value in value_query_result:\n",
    "    col_value[ind_val] = value\n",
    "    ind_val += 1\n",
    "    \n",
    "# create dataframe from dictionary of dictionaries\n",
    "corp_df = pd.DataFrame.from_dict(data = col_value, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate entity address\n",
    "corp_df.loc[:,'EntityAddr'] = [', '.join((str(a),str(b),str(c),str(d))) for a,b,c,d in zip(corp_df['Addr1'],corp_df['Addr2'],corp_df['City'],corp_df['State'])]\n",
    "# double comma problem\n",
    "# append zip to address with no comma\n",
    "corp_df.loc[:,'EntityAddr'] = [' '.join((str(a),str(b))) for a,b in zip(corp_df['EntityAddr'],corp_df['PostalCode'])]\n",
    "corp_df.loc[:,'EntityAddr'] = corp_df.EntityAddr.str.strip()\n",
    "\n",
    "# concatenate agent address\n",
    "corp_df.loc[:,'AgentAddr'] = [', '.join((str(a),str(b),str(c),str(d))) for a,b,c,d in zip(corp_df['AgentAddr1'],corp_df['AgentAddr2'],corp_df['AgentCity'],corp_df['AgentState'])]\n",
    "# append zip to agent address\n",
    "corp_df.loc[:,'AgentAddr'] = [' '.join((str(a),str(b))) for a,b in zip(corp_df['AgentAddr'],corp_df['AgentPostalCode'])]\n",
    "corp_df.loc[:,'AgentAddr'] = corp_df.AgentAddr.str.strip()\n",
    "\n",
    "# slice\n",
    "corp_df_sliced = corp_df[['_key','DataID','EntityTypeDescriptor','EntityName','EntityAddr','AgentName','AgentAddr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back up\n",
    "corp_df_sliced_dup = corp_df_sliced.copy()\n",
    "corp_df_sliced = corp_df_sliced_dup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# two normalization methods to deal with failures of the normalize_address_record() method\n",
    "\n",
    "def normalize_dict(x):\n",
    "    try:\n",
    "        x_1=normalize_address_record(x)\n",
    "    except:\n",
    "        y=corp_df_sliced.index[corp_df_sliced['EntityAddr'] == x]\n",
    "        addr1=corp_df.loc[y,'Addr1'].to_string()\n",
    "        addr2=corp_df.loc[y,'Addr2'].to_string()\n",
    "        city=corp_df.loc[y,'City'].to_string()\n",
    "        state=corp_df.loc[y,'State'].to_string()\n",
    "        pc=corp_df.loc[y,'PostalCode'].to_string()\n",
    "        x_1 = {'address_line_1': addr1, 'address_line_2': addr2, 'city': city, 'state': state, 'postal_code': pc}\n",
    "        pass\n",
    "    return x_1\n",
    "\n",
    "def normalize_concat(x):\n",
    "    if(x==', , ,'):\n",
    "        x_1 = None\n",
    "    else:\n",
    "        try:\n",
    "            y = normalize_address_record(x)\n",
    "            try:\n",
    "                y_0 = ', '.join([y['address_line_1'],y['address_line_2'],y['city'],y['state']])\n",
    "            except:\n",
    "                y_0 = ', '.join([y['address_line_1'],y['city'],y['state']])\n",
    "            y_1 = ' '.join((y_0,y['postal_code']))\n",
    "            x_1 = y_1.strip()\n",
    "        except:\n",
    "            x_1 = x\n",
    "            pass\n",
    "    return x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\0-MSRP22\\NetworkCode\\lib\\site-packages\\pandas\\core\\indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    }
   ],
   "source": [
    "# filtering out non-profit organizations\n",
    "corp_profit = corp_df_sliced[corp_df_sliced.EntityTypeDescriptor != 'Nonprofit Corporation']\n",
    "\n",
    "#normalize addresses\n",
    "corp_profit.loc[:,'EntityAddr']=corp_profit.loc[:,'EntityAddr'].apply(lambda x:normalize_concat(x))\n",
    "corp_profit.loc[:,'AgentAddr']=corp_profit.loc[:,'AgentAddr'].apply(lambda x:normalize_concat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name standardization method\n",
    "\n",
    "text2num = {\"ONE\":\"1\",'TWO':'2','THREE':'3','FOUR':'4','FIVE':'5','SIX':'6','SEVEN':'7','EIGHT':'8','NINE':'9',' TEN':' 10',\n",
    "           'TEN ':'10 ',\"ELEVEN\":\"11\",'TWELVE':'12','THIRTEEN':'13','FOURTEEN':'14','FIFTEEN':'15','SIXTEEN':'16','SEVENTEEN':'17',\n",
    "            'EIGHTEEN':'18','NINETEEN':'19','TWENTY':'20','THIRTY':'30','FORTY':'40','FIFTY':'50','SIXTY':'60',\n",
    "            'SEVENTY':'70','EIGHTY':'80','NINETY':'90'}\n",
    "\n",
    "def normalize_name(x):\n",
    "    # remove special characters\n",
    "    y = re.sub(r'[^\\w\\s]','', x)\n",
    "    # substitute words for numbers with numbers\n",
    "    for text, num in text2num.items():\n",
    "        y = y.replace(text, num)\n",
    "    y = y.strip()\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back up\n",
    "corp_normalized = corp_profit.copy()\n",
    "\n",
    "# standardize names \n",
    "corp_normalized.loc[:,'EntityName']=corp_normalized.loc[:,'EntityName'].apply(lambda x:normalize_name(x))\n",
    "corp_normalized.loc[:,'AgentName']=corp_normalized.loc[:,'AgentName'].apply(lambda x:normalize_name(x))\n",
    "\n",
    "# create json and write it to an existing empty json file\n",
    "result = corp_normalized.to_json(orient=\"records\")\n",
    "parsed = json.loads(result)\n",
    "corp_json = json.dumps(parsed, indent=4) \n",
    "\n",
    "with open('../corp.json', 'w') as outfile:\n",
    "    outfile.write(corp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json to jsonl with jq because file is big, or else it throws error: https://stedolan.github.io/jq/\n",
    "# from command line:\n",
    "# jq -c \".[]\" inputFile.json > outputFile.jsonl\n",
    "\n",
    "# from command line:\n",
    "# arangoimport --file '''path to corp.jsonl''' --collection corp_data_processed --create-collection true --type jsonl --server.database corp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** end of ONLY RUN ONCE for corp_data *****\n",
    "\n",
    "# ***** start of ONLY RUN ONCE for corp_individual *****\n",
    "# process the raw corp_individual collection, including normalization of names, addresses, and slicing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the values of the corp_individual collection\n",
    "indiv_aql = \"FOR x IN corp_individual RETURN x\"\n",
    "indiv_query_result = corp_db.AQLQuery(indiv_aql,rawResults=True, batchSize = 1000)\n",
    "col_indiv = {}\n",
    "ind_indiv = 0\n",
    "\n",
    "for indiv in indiv_query_result:\n",
    "    col_indiv[ind_indiv] = indiv\n",
    "    ind_indiv += 1\n",
    "\n",
    "indiv_df = pd.DataFrame.from_dict(data = col_indiv, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate business address\n",
    "indiv_df.loc[:,'BusAddr'] = [', '.join((str(a),str(b),str(c))) for a,b,c in zip(indiv_df['BusAddr1'],indiv_df['BusCity'],indiv_df['BusState'])]\n",
    "# append zip to address with no comma\n",
    "indiv_df.loc[:,'BusAddr'] = [' '.join((str(a),str(b))) for a,b in zip(indiv_df['BusAddr'],indiv_df['BusPostalCode'])]\n",
    "indiv_df.loc[:,'BusAddr'] = indiv_df.BusAddr.str.strip()\n",
    "\n",
    "# concatenate residential address (although currently unused)\n",
    "indiv_df.loc[:,'ResAddr'] = [', '.join((str(a),str(b),str(c))) for a,b,c in zip(indiv_df['ResAddr1'],indiv_df['ResCity'],indiv_df['ResState'])]\n",
    "# append zip to address with no comma\n",
    "indiv_df.loc[:,'ResAddr'] = [' '.join((str(a),str(b))) for a,b in zip(indiv_df['ResAddr'],indiv_df['ResPostalCode'])]\n",
    "indiv_df.loc[:,'ResAddr'] = indiv_df.ResAddr.str.strip()\n",
    "\n",
    "# concatenate name\n",
    "indiv_df.loc[:,'IndividualName'] = [' '.join((str(a),str(b),str(c))) for a,b,c in zip(indiv_df['LastName'],indiv_df['FirstName'],indiv_df['MiddleName'])]\n",
    "indiv_df.loc[:,'IndividualName'] = indiv_df.IndividualName.str.strip()\n",
    "indiv_df.loc[:,'IndividualNameShort'] = [' '.join((str(a),str(b))) for a,b in zip(indiv_df['LastName'],indiv_df['FirstName'])]\n",
    "\n",
    "# slice\n",
    "indiv_df_sliced = indiv_df[['_key','DataID','IndividualName','IndividualNameShort','BusAddr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\0-MSRP22\\NetworkCode\\lib\\site-packages\\pandas\\core\\indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    }
   ],
   "source": [
    "# normalize business address\n",
    "indiv_df_sliced.loc[:,'BusAddr']=indiv_df_sliced.loc[:,'BusAddr'].apply(lambda x:normalize_concat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json and write it to an existing empty json file\n",
    "result = indiv_df_sliced.to_json(orient=\"records\")\n",
    "parsed = json.loads(result)\n",
    "indiv_json = json.dumps(parsed, indent=4) \n",
    "\n",
    "with open('../indiv_r.json', 'w') as outfile:\n",
    "    outfile.write(indiv_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json to jsonl with jq because file is big, or else it throws error: https://stedolan.github.io/jq/\n",
    "# from command line:\n",
    "# jq -c \".[]\" inputFile.json > outputFile.jsonl\n",
    "\n",
    "# from command line:\n",
    "# arangoimport --file '''path to indiv.jsonl''' --collection indiv_data_processed --create-collection true --type jsonl --server.database corp_data\n",
    "\n",
    "# ***** end of ONLY RUN ONCE for corp_individual *****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NetworkCode",
   "language": "python",
   "name": "networkcode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "137c83b3478ffedff7431332c4108c29fc5733a37d136ef8c89fd7a45cc0790b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
